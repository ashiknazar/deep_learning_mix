{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Linear Functions Don't Help Neural Networks Learn Complex Patterns\n",
    "\n",
    "**The Key Idea**:  \n",
    "When activation functions in a neural network are linear, the network can be simplified to a **single-layer linear model**, no matter how many layers you stack. This significantly limits the network's ability to learn complex patterns.\n",
    "\n",
    "### Why Does This Happen?\n",
    "\n",
    "1. **Linear Activation Functions**:  \n",
    "   A linear activation function has the form $f(x) = ax + b$, where $a$ and $b$ are constants. This means the output is a linear transformation of the input.\n",
    "\n",
    "2. **Stacking Linear Layers**:  \n",
    "   Let's say you have a neural network with two layers:\n",
    "   - The first layer produces an output: $y_1 = W_1 x + b_1$\n",
    "   - The second layer produces an output: $y_2 = W_2 y_1 + b_2$\n",
    "   \n",
    "   Now, if both activation functions in these layers are linear, we can substitute $y_1$ into $y_2$:\n",
    "   \\[\n",
    "   y_2 = W_2 (W_1 x + b_1) + b_2\n",
    "   \\]\n",
    "   Simplifying:\n",
    "   \\[\n",
    "   y_2 = (W_2 W_1) x + (W_2 b_1 + b_2)\n",
    "   \\]\n",
    "   Notice that the result is still a linear function of $x$. The network with two layers is equivalent to a **single-layer** network with weights $W_2 W_1$ and bias $W_2 b_1 + b_2$.\n",
    "\n",
    "3. **Effect on Learning**:  \n",
    "   The problem here is that **stacking layers of linear transformations doesn't add complexity** to the model. No matter how many layers you add, the overall transformation remains a linear function. This means the network can't learn or model more complex, non-linear relationships between input and output.\n",
    "\n",
    "4. **Non-linear Activation Functions**:  \n",
    "   In contrast, if you use **non-linear activation functions** (like ReLU, Sigmoid, or Tanh), each layer introduces non-linearity into the model. The composition of these non-linear transformations enables the network to model complex patterns, learn hierarchical representations, and handle more sophisticated tasks (like image classification, natural language processing, etc.).\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **Linear activation functions** cause the network to collapse into a single-layer linear model, regardless of the number of layers.\n",
    "- **Non-linear activation functions** enable the network to learn complex, non-linear mappings, which is why they are essential in deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalization: Composition of Non-linear Functions\n",
    "\n",
    "**Statement**:  \n",
    "The composition of two non-linear functions $f_1(x)$ and $f_2(x)$ results in a **non-linear function**. This holds true for most non-linear functions unless there is a specific structure that cancels out the non-linearity.\n",
    "\n",
    "### Why Does This Happen?\n",
    "\n",
    "1. **Non-linearity of a function**:  \n",
    "   A function $f(x)$ is non-linear if it doesn't satisfy the properties of linearity:\n",
    "   - **Additivity**: $f(x_1 + x_2) = f(x_1) + f(x_2)$\n",
    "   - **Homogeneity**: $f(\\alpha x) = \\alpha f(x)$, for any scalar $\\alpha$\n",
    "   \n",
    "   Non-linear functions do not satisfy one or both of these conditions.\n",
    "\n",
    "2. **Composition of Functions**:  \n",
    "   When you compose two functions $f_1(x)$ and $f_2(x)$, the result is $f_2(f_1(x))$. Even if one function is non-linear, the composition generally leads to a non-linear result because:\n",
    "   - If $f_1(x)$ is non-linear, it produces outputs that are not simply scaled or shifted versions of the input.\n",
    "   - Applying $f_2(x)$ to this non-linear output $f_1(x)$ often results in further distortion (non-linearity) of the data.\n",
    "\n",
    "3. **Mathematical Example**:  \n",
    "   Consider two non-linear functions:\n",
    "   - $f_1(x) = x^2$ (a quadratic function)\n",
    "   - $f_2(x) = e^x$ (an exponential function)\n",
    "\n",
    "   The composition $f_2(f_1(x)) = e^{x^2}$ is clearly non-linear because the exponential of a quadratic function is not a linear function. It grows exponentially, and the relationship between the input and output is highly non-linear.\n",
    "\n",
    "4. **Key Insight**:  \n",
    "   The composition of two non-linear functions is **generally non-linear** because the way the functions interact doesn’t “cancel out” the non-linearity:\n",
    "   - Non-linear transformations (like squaring, exponentiating, or applying trigonometric functions) introduce new behaviors (e.g., curvatures, oscillations) that cannot be replicated by a single linear transformation.\n",
    "   - **Linear transformations** (such as scaling or shifting) are additive and homogeneous, so applying one on top of the other simply results in another linear transformation, not a more complex result.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- **If $f_1(x)$ and $f_2(x)$ are both non-linear, the composition $f_2(f_1(x))$ is typically non-linear**, because applying a non-linear function to a non-linear transformation generally produces a result that cannot be expressed as a simple linear function of the original input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
