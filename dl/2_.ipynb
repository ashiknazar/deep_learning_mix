{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nn.Module\n",
    "- nn.Module is a base class in PyTorch that is used to define neural network models. All custom neural network layers or models in PyTorch typically inherit from nn.Module. It provides useful methods and attributes to facilitate model creation, training, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here's a brief breakdown of what nn.Module provides:\n",
    "\n",
    "1. Layer Initialization: When you create a subclass of nn.Module, you define your network layers (such as nn.Linear, nn.Conv2d, etc.) inside the __init__ method.\n",
    "\n",
    "2. Forward Pass: The forward method is where you define how the input data flows through the layers of the network. This method is called when you pass data through the model.\n",
    "\n",
    "3. Parameter Management: nn.Module automatically tracks the model's parameters (e.g., weights and biases) and allows you to update them during training using optimizers like torch.optim.\n",
    "\n",
    "4. Device Management: nn.Module can automatically transfer model parameters between CPU and GPU using .to(device)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch workflow module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- get data ( turn into tensors)\n",
    "- build or pick a pretrained model\n",
    "- fit model\n",
    "- evaluate\n",
    "- imporove\n",
    "- save and reload\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "#nn contains all of pytorchs building blocks for neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "weight=0.7\n",
    "bias=0.3\n",
    "start=0\n",
    "end=1\n",
    "step=0.02\n",
    "X=torch.arange(start,end,step).unsqueeze(dim=1)\n",
    "y=weight*X+bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x[:10],y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training dataset\n",
    "- validation dataset\n",
    "- test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- training doesnt see validation/test data\n",
    "-   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "train_split=int(0.8*len(X))\n",
    "x_train,y_train=X[:train_split],y[:train_split]\n",
    "x_test,y_test=X[train_split:],y[train_split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ploting data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def plot_predictions(train_data=x_train,train_labels=y_train,test_data=x_test,test_labels=y_test,predictions=None):\n",
    "     \n",
    "     plt.figure(figsize=(10,7))\n",
    "     plt.scatter(train_data,train_labels,c=\"b\",s=4,label=\"training_data\")\n",
    "     plt.scatter(test_data,test_labels,c=\"g\",)\n",
    "     if predictions is not None:\n",
    "        plt.scatter(test_data,predictions,c=\"r\")\n",
    "    plt.legend(prop={\"size\":14})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n",
    "        self.bias=nn.Parameter(torch.randn(1,requires_grad=True,dtype=torch.float))\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return self.weight*x+self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - In PyTorch, setting requires_grad=True on a tensor means that the tensor will track operations performed on it for the purpose of automatic differentiation. This is useful when you want to optimize or update the tensor's values during training in a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PyTorch, torch.nn.Parameter is a subclass of torch.Tensor that is specifically used to define parameters in neural networks. A Parameter is a tensor that is automatically registered as a parameter of a model (i.e., it is included in the model's parameters() list, which makes it easy to optimize during training).\n",
    "\n",
    "- Key Points about torch.nn.Parameter:\n",
    "- Automatic Inclusion in Model Parameters:\n",
    "\n",
    "  - When you use torch.nn.Parameter to define a tensor, it is automatically treated as a model parameter that should be optimized during training. This means it is added to the list of parameters returned by model.parameters().\n",
    "- Gradient Tracking:\n",
    "\n",
    "  - torch.nn.Parameter automatically has requires_grad=True, so it will track operations for automatic differentiation. This is useful when defining weights, biases, or other parameters in a neural network.\n",
    "- How to Use It:\n",
    "\n",
    "  - You can create a Parameter and assign it to the model. It is commonly used to define the weights or bias terms in layers like nn.Linear, nn.Conv2d, etc., but you can also use it explicitly in custom layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "model=LinEARegressionModel()\n",
    "model.parameters()\n",
    "model.state_dict()\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.inference_mode(), which is a feature introduced in PyTorch 2.0. It's designed to disable gradient tracking during inference, similar to torch.no_grad(), but it provides additional performance optimizations.\n",
    "- Purpose: torch.inference_mode() is used to disable gradient computation during inference, which can improve performance. While torch.no_grad() does this as well, inference_mode() provides even more efficient memory usage and can further accelerate inference.\n",
    "- Usage: It's particularly useful when you are running a model in a production or deployment setting where you do not need gradients, and you want to ensure optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "loss_fn=nn.L1Loss()\n",
    "optimizer=torch.optim.SGD(params=model.arameters(),lr=0.01)\n",
    "\n",
    "epochs=200\n",
    "torch.manual_seed(42)\n",
    "for epoch in range(epochs):\n",
    "    model.train() #set model to training mode set all parameters to require gradients\n",
    "    y_pred=model(x_train)\n",
    "    loss=loss_fn(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()#backward propagation on loss wrt parameters\n",
    "    optimizer.step()#step the optimizer(perform gradient descent)\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "    test_pred=model1(x_test)\n",
    "    test_loss=loss_fn(test_pred,y_test)\n",
    "    if epoch % 10 ==0:\n",
    "        print(f\"epoch:{epoch} Loss:{loss} test  Loss : {test_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The optimizer.zero_grad() function in PyTorch is used to clear (zero out) the gradients of all model parameters before performing the backpropagation step.\n",
    "-In the context of training a neural network, gradients are accumulated in PyTorch by default. After each backward pass, gradients are stored in each parameterâ€™s ``.grad `` attribute, which are then used by the optimizer to update the model's weights. If you don't clear the gradients before each new backward pass, the gradients will accumulate from the previous iterations, leading to incorrect updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Math Example: Relevance of Clearing Gradients\n",
    "\n",
    "Let's consider a simple scenario to understand the relevance of clearing gradients.\n",
    "\n",
    "#### Setup:\n",
    "We have:\n",
    "- $w$ = weight parameter\n",
    "- $x$ = input value\n",
    "- $y$ = target value (ground truth)\n",
    "\n",
    "The loss function is the Mean Squared Error (MSE):\n",
    "$$\n",
    "\\text{Loss} = \\frac{1}{2} \\cdot (y - w \\cdot x)^2\n",
    "$$\n",
    "\n",
    "#### Step-by-Step Example:\n",
    "\n",
    "1. **First Iteration:**\n",
    "    - Let $w = 2$, $x = 3$, and $y = 6$.\n",
    "    - Compute the model's output: $\\hat{y} = w \\cdot x = 2 \\cdot 3 = 6$.\n",
    "    - Compute the loss: \n",
    "    $$\n",
    "    \\text{Loss} = \\frac{1}{2} \\cdot (6 - 6)^2 = 0\n",
    "    $$\n",
    "    - Compute the gradient of the loss with respect to $w$:\n",
    "    $$\n",
    "    \\frac{\\partial \\text{Loss}}{\\partial w} = x \\cdot (w \\cdot x - y) = 3 \\cdot (6 - 6) = 0\n",
    "    $$\n",
    "\n",
    "2. **Second Iteration (without clearing gradients):**\n",
    "    - Now, change the input to $x = 2$, keeping $y = 6$ and $w = 2$.\n",
    "    - Compute the model's output: $\\hat{y} = w \\cdot x = 2 \\cdot 2 = 4$.\n",
    "    - Compute the loss: \n",
    "    $$\n",
    "    \\text{Loss} = \\frac{1}{2} \\cdot (6 - 4)^2 = 2\n",
    "    $$\n",
    "    - Compute the gradient:\n",
    "    $$\n",
    "    \\frac{\\partial \\text{Loss}}{\\partial w} = x \\cdot (w \\cdot x - y) = 2 \\cdot (4 - 6) = -4\n",
    "    $$\n",
    "\n",
    "    If gradients are **not cleared**, the gradient accumulates:\n",
    "    $$\n",
    "    \\text{Accumulated Gradient} = 0 + (-4) = -4\n",
    "    $$\n",
    "\n",
    "    The optimizer would use this accumulated gradient for weight update.\n",
    "\n",
    "3. **Second Iteration (with clearing gradients):**\n",
    "    - If we **clear the gradients** using `optimizer.zero_grad()`, the gradient from the first iteration (which was 0) is discarded, and the optimizer will only use the gradient from the second iteration:\n",
    "    $$\n",
    "    \\text{Gradient for Update} = -4\n",
    "    $$\n",
    "\n",
    "#### Conclusion:\n",
    "\n",
    "- **Without clearing gradients**, the optimizer accumulates gradients, causing incorrect updates.\n",
    "- **With clearing gradients**, each iteration starts with fresh gradients, leading to correct and intended weight updates.\n",
    "\n",
    "This illustrates why clearing gradients is crucial in neural network training for proper optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model.eval()\n",
    "- turns off different settings in the model not needed for evaluation/testing ,dropout/batch norm layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### disabling gradient calculations and weight updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred=\n",
    "\n",
    "with torch.inference_mode():\n",
    "    y_preds=model(x_test)\n",
    "\n",
    "y_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
