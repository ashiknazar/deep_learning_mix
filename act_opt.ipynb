{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Activation Functions and Optimization Algorithms**\n",
    "\n",
    "## **Activation Functions**\n",
    "\n",
    "Activation functions are mathematical equations that determine the output of a neural network layer. They introduce non-linearity, helping neural networks learn complex patterns.\n",
    "\n",
    "### **1. Sigmoid**\n",
    "- **Equation**:  \n",
    "  $$f(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "- **Range**: (0, 1)  \n",
    "- **Usage**: Output layers in binary classification.  \n",
    "- **Pros**:\n",
    "  - Outputs interpretable as probabilities.\n",
    "- **Cons**:\n",
    "  - Vanishing gradient problem.\n",
    "  - Not zero-centered.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Tanh (Hyperbolic Tangent)**\n",
    "- **Equation**:  \n",
    "  $$f(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "- **Range**: (-1, 1)  \n",
    "- **Usage**: Hidden layers for zero-centered outputs.  \n",
    "- **Pros**: Zero-centered.  \n",
    "- **Cons**: Vanishing gradients for large inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. ReLU (Rectified Linear Unit)**\n",
    "- **Equation**:  \n",
    "  $$f(x) = \\max(0, x)$$\n",
    "- **Range**: [0, ∞)  \n",
    "- **Usage**: Hidden layers in most modern architectures.  \n",
    "- **Pros**:\n",
    "  - Computationally efficient.\n",
    "  - Mitigates vanishing gradient issues.\n",
    "- **Cons**: Dead neurons for  \\(x \\leq 0\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Leaky ReLU**\n",
    "- **Equation**:  \n",
    "  $$f(x) = \\begin{cases} \n",
    "      x & \\text{if } x > 0, \\\\\n",
    "      \\alpha x & \\text{if } x \\leq 0\n",
    "   \\end{cases}$$\n",
    "  where $\\alpha$ is a small positive constant (e.g., 0.01).  \n",
    "- **Range**: (-∞, ∞)  \n",
    "- **Usage**: Prevents dead neurons.  \n",
    "- **Pros**: Allows small gradients for negative inputs.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Softmax**\n",
    "- **Equation**:  \n",
    "  $$f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^N e^{x_j}}$$\n",
    "- **Range**: (0, 1), where all outputs sum to 1.  \n",
    "- **Usage**: Multi-class classification outputs.  \n",
    "\n",
    "---\n",
    "\n",
    "### **6. Swish**\n",
    "- **Equation**:  \n",
    "  $$f(x) = x \\cdot \\text{sigmoid}(x)$$\n",
    "- **Range**: (-∞, ∞)  \n",
    "- **Usage**: Deep learning tasks for smoother gradients.  \n",
    "\n",
    "---\n",
    "____\n",
    "____\n",
    "## **Optimization Algorithms**\n",
    "\n",
    "Optimization algorithms minimize the loss function by updating weights and biases in the neural network.\n",
    "\n",
    "### **1. Gradient Descent**\n",
    "- **Update Rule**:  \n",
    "  $$\\theta = \\theta - \\eta \\cdot \\nabla L(\\theta)$$\n",
    "- **Variants**:\n",
    "  - Batch Gradient Descent (entire dataset per update).\n",
    "  - Stochastic Gradient Descent (SGD, one sample per update).\n",
    "  - Mini-Batch Gradient Descent (small batches of data).\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Momentum**\n",
    "- **Update Rule**:\n",
    "  $$v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta)$$  \n",
    "  $$\\theta = \\theta + v_t$$\n",
    "- **Benefit**: Speeds up convergence and reduces oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. RMSprop**\n",
    "- **Update Rule**:\n",
    "  $$G_t = \\beta G_{t-1} + (1 - \\beta) \\nabla L(\\theta)^2$$  \n",
    "  $$\\theta = \\theta - \\frac{\\eta}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla L(\\theta)$$\n",
    "- **Benefit**: Works well for non-stationary objectives.\n",
    "- Non-stationary objectives refer to situations where the data distribution or loss landscape changes during training. This is common in deep learning as different parts of the network adjust their weights.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Adam (Adaptive Moment Estimation)**\n",
    "- **Update Rule**:\n",
    "  $$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L(\\theta)$$  \n",
    "  $$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L(\\theta))^2$$  \n",
    "  $$\\theta = \\theta - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\cdot \\hat{m}_t$$\n",
    "- **Benefits**:\n",
    "  - Adaptive learning rate.\n",
    "  - Fast convergence.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Nesterov Accelerated Gradient (NAG)**\n",
    "- **Update Rule**:\n",
    "  $$v_t = \\beta v_{t-1} - \\eta \\nabla L(\\theta + \\beta v_{t-1})$$\n",
    "- **Benefit**: Anticipates future gradients.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. AdaGrad**\n",
    "- **Update Rule**:\n",
    "  $$\\theta = \\theta - \\frac{\\eta}{\\sqrt{G_{ii} + \\epsilon}} \\cdot \\nabla L(\\theta)$$\n",
    "- **Benefit**: Suitable for sparse data.\n",
    "- **Drawback**: Learning rate decays too fast.\n",
    "\n",
    "---\n",
    "\n",
    "### **7. AdaDelta**\n",
    "- **Update Rule**:\n",
    "  $$\\theta = \\theta - \\frac{\\eta \\cdot \\nabla L(\\theta)}{\\sqrt{E[\\nabla L(\\theta)^2] + \\epsilon}}$$\n",
    "- **Benefit**: Eliminates the need for a global learning rate.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. AMSGrad**\n",
    "- **Description**: A variant of Adam using the maximum of past squared gradients for stability.  \n",
    "\n",
    "---\n",
    "\n",
    "### **9. LAMB (Layer-wise Adaptive Moments)**\n",
    "- **Description**: Used for large-batch training in distributed environments.  \n",
    "\n",
    "---\n",
    "\n",
    "### **10. Cosine Annealing with Warm Restarts (SGDR)**\n",
    "- **Learning Rate Schedule**:\n",
    "  $$\\eta_t = \\eta_{\\text{min}} + 0.5 (\\eta_{\\text{max}} - \\eta_{\\text{min}}) (1 + \\cos(\\frac{t}{T} \\pi))$$\n",
    "- **Benefit**: Avoids local minima and helps explore the loss landscape.\n",
    "\n",
    "---\n",
    "\n",
    "This covers the most commonly used **activation functions** and **optimization algorithms**. Let me know if you need further clarifications or examples!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
