{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecf10c6-c85e-40b8-86aa-eb0e2772a217",
   "metadata": {},
   "source": [
    "# AI Syllabus\r\n",
    "\r\n",
    "## <u>OpenCV</u>\r\n",
    "1. Reading images and videos\r\n",
    "2. Drawing shapes\r\n",
    "3. Mouse events\r\n",
    "4. Operations\r\n",
    "5. Trackbars\r\n",
    "6. HSV\r\n",
    "7. Thresholding\r\n",
    "8. Smoothing\r\n",
    "9. Image gradients\r\n",
    "10. Image pyramids\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## <u>The Math Behind Neural Networks</u>\r\n",
    "\r\n",
    "### 1. Introduction to Neural Networks\r\n",
    "- Overview of neural networks and their applications\r\n",
    "- Basic architecture (neurons, layers, activation functions)\r\n",
    "\r\n",
    "### 2. Linear Algebra Fundamentals\r\n",
    "- Vectors and matrices\r\n",
    "- Matrix operations (addition, multiplication, transposition)\r\n",
    "- Dot product and its geometric interpretation\r\n",
    "\r\n",
    "### 3. Calculus Basics\r\n",
    "- Functions and limits\r\n",
    "- Derivatives and gradients\r\n",
    "- Partial derivatives\r\n",
    "\r\n",
    "### 4. Loss Functions\r\n",
    "- Introduction to loss functions (Mean Squared Error, Cross-Entropy)\r\n",
    "- Understanding how loss functions guide learning\r\n",
    "\r\n",
    "### 5. Gradient Descent\r\n",
    "- Concept of optimization and finding minima\r\n",
    "- Stochastic vs. batch gradient descent\r\n",
    "- Learning rate and its impact\r\n",
    "\r\n",
    "### 6. Backpropagation Algorithm\r\n",
    "- Chain rule of calculus\r\n",
    "- Step-by-step derivation of backpropagation\r\n",
    "- Intuition behind weight updates\r\n",
    "\r\n",
    "### 7. Regularization Techniques\r\n",
    "- Overfitting vs. underfitting\r\n",
    "- L1 and L2 regularization\r\n",
    "- Dropout and its mathematical reasoning\r\n",
    "\r\n",
    "### 8. Advanced Topics in Optimization\r\n",
    "- Momentum and adaptive learning rates (Adam, RMSprop)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## <u>TensorFlow Basics</u>\r\n",
    "- Scalars, vectors\r\n",
    "  - Constant, shape, ndim\r\n",
    "- Data types, casting\r\n",
    "- Multi-dimensional arrays\r\n",
    "  - `convert_to_arrays`, `eye`, `fill`, `ones`, `ones_like`, `zeros`\r\n",
    "- Random\r\n",
    "  - Normal, uniform\r\n",
    "- Math\r\n",
    "  - `abs`, `sqrt`, `add`, `mul`, `argmax`, `argmin`, `reduce_sum`, `reduce_min`, `reduce_max`\r\n",
    "  - `linalg.matmul`, `transpose`, `adjoint`, `svd`\r\n",
    "  - `einsum`\r\n",
    "- Expand dims\r\n",
    "- Squeeze\r\n",
    "- Reshape, concat, stack\r\n",
    "- Padding\r\n",
    "- Ragged tensors\r\n",
    "- Ragged boolean mask\r\n",
    "- Sparse tensor\r\n",
    "- String operations\r\n",
    "- Variables\r\n",
    "  - `assign`, `assign_sub`, `assign_add`\r\n",
    "- Random generator\r\n",
    "- Shuffle order\r\n",
    "- TensorFlow statistics\r\n",
    "  - `tensorflow_probability.stats.variance`\r\n",
    "- One-hot encoding\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## <u>PyTorch Basics</u>\r\n",
    "- `torch.tensor`\r\n",
    "- Random tensors\r\n",
    "- `zeros`, `ones`, `zeros_like`\r\n",
    "- NumPy to tensor\r\n",
    "- `arange`\r\n",
    "- `requires_grad`\r\n",
    "- Math operations\r\n",
    "- Aggregations\r\n",
    "- `argmin`, `argmax`\r\n",
    "- Reshaping, stacking, squeezing, unsqueezing\r\n",
    "- Permute, indexing\r\n",
    "- Reproducibility\r\n",
    "- GPU support\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## <u>Keras Basics Syllabus</u>\r\n",
    "\r\n",
    "### Course Overview\r\n",
    "This course introduces Keras, a high-level neural networks API, to build and train deep learning models. By thnd of the course, students will be able to implement and evaluate basic neural network architectures.\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Week 1: Introduction to Deep Learning and Keras\r\n",
    "- **Lecture Topics:**\r\n",
    "  - Overview of deep learning and its applications\r\n",
    "  - Introduction to Keras: What is it and why use it?\r\n",
    "  - Installing Keras and its dependencies\r\n",
    "- **Ha-on:**\r\n",
    "  - Setting up the environment (TensorFlow, Keras)\r\n",
    "  - Simple Keras program: \"Hello, World!\" with Keras\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Week 2: Understanding Keras Basics\r\n",
    "- **Lecture Topics:**\r\n",
    "  - Keras layers and models\r\n",
    "  - Sequential vs. Functional API\r\n",
    "  - Common layers: Dense, Activation, Dropout\r",
    "*Hands-on:**\r\n",
    "  - Creating a simple feedforward neural network using Sequential API\r\n",
    "  - Visualizing the model architecture\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Week 3: Data Preprocessing\r\n",
    "- **Lecture Topics:**\r\n",
    "  - Importance of data preprocessing\r\n",
    "  - Techniques: normalization, scaling, one-hot encoding\r\n",
    "  - Using `ImageDataGeator` for image data\r\n",
    "- **Hands-on:**\r\n",
    "  - Preparing datasets for training (e.g., MNIST, CIFAR-10)\r\n",
    "  - Implementing data augmentation\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Week 4: Building and Training Models\r\n",
    "- **Lecture Topics:**\r\n",
    "  - Compiling models: loss functions, optimizers, and metrics\r\n",
    "  - Training models: fit,aluate, and predict methods\r\n",
    "- **Hands-on:**\r\n",
    "  - Training a model on a sample dataset\r\n",
    "  - Evaluating model performance and visualizing results\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### Week 5: Model Evaluation and Tuning\r\n",
    "- **Lecture Topics:**\r\n",
    "  - Overfitting and underfitting\r\n",
    "  - Techniques fmodel improvement: regularization, dropout\r\n",
    "  - Hyperparameter tuning basics\r\n",
    "- **Hands-on:**\r\n",
    "  - Implementing callbacks (EarlyStopping, ModelCheckpoint)\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "## <u>Neural Network Architectures</u>\r\n",
    "\r\n",
    "### 1. Feedforward Neural Networks (FNN)\r\n",
    "- **Description**: The simplest type of artificial neural network where connections between the nodes do not form cycles.\r\n",
    "- **Use Cases**: Regression, classification tasks.\r\n",
    "\r\n",
    "### 2. Convolutional Neural Networks (CNN)\r\n",
    "- **Description**: Designed for processing structured grid data like images. They use convolutional layers to automatically learn spatial hierarchies.\r\n",
    "- **Key Components**:\r\n",
    "  - Convolutional layers\r\n",
    "  - Pooling layers\r\n",
    "  - Fully connected layers\r\n",
    "- **Use Cases**: Image recognition, object detection, video analysis.\r\n",
    "\r\n",
    "### 3. Recurrent Neural Networks (RNN)\r\n",
    "- **Description**: Designed for sequential data, RNNs have connections that can loop back, allowing them to maintain memory of previous inputs.\r\n",
    "- **Use Cases**: Natural language processing, time series prediction.\r\n",
    "\r\n",
    "### 4. Long Short-Term Memory Networks (LSTM)\r\n",
    "- **Description**: A type of RNN designed to remember long-term dependencies and avoid issues like vanishing gradients.\r\n",
    "- **Use Cases**: Language modeling, speech recognition, sequence prediction.\r\n",
    "\r\n",
    "### 5. Gated Recurrent Units (GRU)\r\n",
    "- **Description**: A simplified version of LSTMs that combines the forget and input gates into a single update gate, making them computationally efficient.\r\n",
    "- **Use Cases**: Similar to LSTMs, used in various sequence modeling tasks.\r\n",
    "\r\n",
    "### 6. Autoencoders\r\n",
    "- **Description**: A type of unsupervised learning model that learns to encode data into a lower-dimensional space and then reconstruct it.\r\n",
    "- **Key Variants**:\r\n",
    "  - Denoising Autoencoders\r\n",
    "  - Variational Autoencoders (VAE)\r\n",
    "- **Use Cases**: Dimensionality reduction, anomaly detection.\r\n",
    "\r\n",
    "### 7. Generative Adversarial Networks (GAN)\r\n",
    "- **Description**: Comprises two neural networks (a generator and a discriminator) that compete against each other to generate new, synthetic instances of data.\r\n",
    "- **Use Cases**: Image generation, data augmentation.\r\n",
    "\r\n",
    "### 8. Transformer Networks\r\n",
    "- **Description**: Based on self-attention mechanisms, transformers excel in handling sequential data without recurrence, making them highly parallelizable.\r\n",
    "- **Key Components**:\r\n",
    "  - Multi-head attention\r\n",
    "  - Positional encoding\r\n",
    "- **Use Cases**: Natural language processing (BERT, GPT), translation tasks.\r\n",
    "\r\n",
    "### 9. Graph Neural Networks (GNN)\r\n",
    "- **Description**: Designed to work with graph-structured data, capturing dependencies and relationships between nodes.\r\n",
    "- **Use Cases**: Social network analysis, recommendation systems.\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
