{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder decoder architecture\n",
    "- input sequence can be passed in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LSTM](images/tr_parra.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![LSTM](images/tr_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Input Embedding\n",
    "\n",
    " ![](images/embspace.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- computer dont understand words\n",
    "- they use nos,vectors,matrices\n",
    "- the idea is to map every words into a space where related words are close to each other (empedding space)\n",
    "- already pretrained embedding space are available \n",
    "    - Glove(Global vectors for word representation)\n",
    "    - word to vectors\n",
    "    - but the same word in different sentence have different meaning(He went to the bank to fish.\"\n",
    "\"He deposited money in the bank.\")(\"I saw the man with the telescope.\",\"I saw the man with the binoculars.\")(aj's dog is a cutie,aj looks like a dog)\n",
    "    -  is the reason to use  <b>Positional Encoder</b> \n",
    "        - vector that gives the context based on position of word in sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![LSTM](images/context.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> pass this into encoder block </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![LSTM](images/enc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention\n",
    "- what part of input should i focus on\n",
    "- how relevant is the i-th word in a sentence is relevant with respect to others words in the sentence\n",
    "\n",
    " ![LSTM](images/atten.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- attention vectors are passed to feed forward networks ,which converts it to a form usable by next encoder/decoder block\n",
    "(the,big,red,dog  -> each vector is applied to fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- during training we feed output french setence to decoder\n",
    "- use embedding and positional encoding \n",
    "- pass this to a decoder block\n",
    "- decoder has 3 main components \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![LSTM](images/dec.png)\n",
    "![LSTM](images/dec_att.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- output of this block is attention vector for every word in english and french sentence\n",
    "- each vector represents the relationship between words in both languages\n",
    "- then we pass it to a feed forward unit\n",
    "- makes the output more digestable by a decoder block or a linear layer(another fnn used to expand the dimentions to suit the no of words in french language)\n",
    "- softmax layer transfer it to a probability distribution\n",
    "- final word is word with highest probability\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>more deep </b>\n",
    " ### <u>Attention</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- too much focus on itself (the,the),its useless \n",
    "- we want interactions (big,dog)\n",
    "- we take 8 such attention vector per word and take weighted average\n",
    "- since we use multiple attention vectors we call it multi-head attention block\n",
    "\n",
    "\n",
    "![self importance](images/self.png)\n",
    "![multi head](images/multi_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- each of the attention nets are independent of each other\n",
    "- so we can use parallelization and pass multiple words at same time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked-attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- while generating the next french word we can use all english words ,but only previous french words.if we use all words from french sentence then there is no learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>more deep</n>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### single headed attention\n",
    "![single head](images/single_headed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### multi headed attention\n",
    "![multi head](images/mul_head.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- after every layer we apply normalization (typically - batch normalization) -> smooths surface and easy to optimize\n",
    "- layer normalization -> normalization across each feature instead of each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    " #### RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RNN](images/rnn1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x -> input <br>\n",
    "h -> hidden state <br>\n",
    "o -> output <br>\n",
    "y -> label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequence to sequence\n",
    "- slow (sequentially,truncated back propagation algorithm) \n",
    "- not sure about context here because here only words present in the previous is only considered\n",
    "  - even bidirectional rnn suffers (beacause it do each direction contexts seperatly and concatinates)  \n",
    "  - so use <B>attention mechanism</B>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Next Page examples](5_example.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
