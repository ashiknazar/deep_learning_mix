{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In an FNN, information moves in one direction—forward—from the input layer through one or more hidden layers to the output layer. Here are some key characteristics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture:\n",
    "- Composed of an input layer, one or more hidden layers, and an output layer.\n",
    "- Neurons in each layer are fully connected to neurons in the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  While the classic feedforward neural network typically consists of fully connected layers, it can also include layers that are not fully connected. \n",
    "   - Convolutional Layers: In convolutional neural networks (CNNs), convolutional layers are used to process spatial data like images. These layers apply filters to local regions of the input, making them not fully connected.\n",
    "\n",
    "   - Dropout Layers: Dropout is a regularization technique that randomly sets a fraction of the neurons to zero during training. This disrupts full connectivity temporarily.\n",
    "\n",
    "   - Pooling Layers: In CNNs, pooling layers downsample feature maps and are not fully connected.\n",
    "\n",
    "   - Residual Connections: In some architectures, like ResNets, there are skip connections that bypass certain layers, breaking the traditional fully connected flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-<b> if random set are chosen then how does they study</b>(for dropout)\n",
    " - <u>Shared Learning</u>: Each neuron that remains active during a training iteration still contributes to the learning process. Over multiple iterations, the neurons experience various combinations of activations, which helps them learn useful features independently of specific others.\n",
    " - <u>Ensemble Effect:</u> Dropout effectively trains many different \"sub-networks\" within the larger network. Each training pass teaches the model to generalize better by making it less reliant on any single set of neurons. This ensemble-like behavior can lead to more robust representations.\n",
    " - <u>Consistent Weight Updates:</u> The weights of all neurons are updated based on the loss calculated from the active neurons during each iteration. Over time, even though different subsets are active, the overall weight updates guide the entire network toward minimizing loss and improving performance.\n",
    " - <u> Exposure to Diverse Patterns: </u>By dropping different neurons each time, the model is exposed to a variety of data patterns. This encourages it to learn features that are useful across different contexts, rather than memorizing specific patterns.\n",
    " - <u>Final Inference:</u> During inference, all neurons are used together, which combines the strengths learned from multiple iterations. The model can leverage the diverse features learned during training for more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[next page rnn](2_rnn.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
