{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize pixel values (0 to 1)\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to 1D\n",
    "    Dense(128, activation='relu'),  # Hidden layer with 128 neurons\n",
    "    Dense(10, activation='softmax') # Output layer with 10 neurons\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Size: Number of samples processed before updating weights.\n",
    "- Epochs: Full passes through the training data.\n",
    "- Validation Split: Portion of data for validation during training.\n",
    "- Callbacks: Automate tasks like saving models or reducing learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "____\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Batch Processing in Deep Learning**\n",
    "\n",
    "In deep learning, **batch processing** refers to dividing the dataset into smaller groups of samples, called **batches**, which are processed sequentially during training. Below are key concepts to understand:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. What Is a Batch?**\n",
    "- A **batch** is a subset of the dataset that is processed by the model in one forward and backward pass.\n",
    "- The size of the batch is defined by the **batch size**.\n",
    "\n",
    "### **Batch Size**\n",
    "- Refers to the number of samples processed at a time.\n",
    "- Common values: `32`, `64`, `128`, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Why Use Batches?**\n",
    "- **Memory Efficiency**: Loading and processing the entire dataset at once might exceed system memory, especially for large datasets.\n",
    "- **Faster Training**: Hardware like GPUs and TPUs can process batches in parallel, speeding up computations.\n",
    "- **Stable Optimization**: Mini-batches balance computational efficiency and the randomness of stochastic gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. How Are Batches Processed?**\n",
    "### **a. Processing Samples in a Batch**\n",
    "- **Parallel Processing**: \n",
    "  - Samples in a batch are processed **independently and in parallel**.\n",
    "  - For example, during matrix operations in a neural network, all samples in the batch are computed simultaneously on GPUs.\n",
    "\n",
    "### **b. Processing Multiple Batches**\n",
    "- **Sequential Processing**:\n",
    "  - Batches are processed one after another in a single training process.\n",
    "  - After processing a batch:\n",
    "    1. Compute **loss**.\n",
    "    2. Perform **backpropagation** to calculate gradients.\n",
    "    3. Update **weights**.\n",
    "  - The next batch uses the updated weights.\n",
    "\n",
    "- **Distributed Training**:\n",
    "  - In multi-GPU setups, different GPUs can process separate batches **in parallel**. Gradients are combined to update shared weights.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Memory Considerations**\n",
    "### **Why Is Training Memory-Intensive?**\n",
    "- **Activations**: Outputs of each layer in the forward pass are stored for use in the backward pass.\n",
    "- **Gradients**: Gradients of all parameters are computed and stored during backpropagation.\n",
    "- **Optimizer States**: Advanced optimizers like Adam maintain additional states (e.g., moving averages of gradients).\n",
    "\n",
    "### **Impact of Batch Size**\n",
    "- **Larger Batch Size**:\n",
    "  - Pros: More efficient on GPUs, faster processing.\n",
    "  - Cons: Requires more memory and may lead to poorer generalization.\n",
    "- **Smaller Batch Size**:\n",
    "  - Pros: Less memory-intensive, better generalization.\n",
    "  - Cons: Slower training due to more weight updates per epoch.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Types of Gradient Descent**\n",
    "### **a. Batch Gradient Descent**\n",
    "- Uses the **entire dataset** as one batch.\n",
    "- Pros: Stable weight updates.\n",
    "- Cons: Memory-intensive, slow for large datasets.\n",
    "\n",
    "### **b. Stochastic Gradient Descent (SGD)**\n",
    "- Processes **one sample** at a time.\n",
    "- Pros: Faster updates, less memory required.\n",
    "- Cons: Noisy and less stable optimization.\n",
    "\n",
    "### **c. Mini-Batch Gradient Descent**\n",
    "- Processes a **subset of samples** at a time (e.g., batch size = 32).\n",
    "- Combines the benefits of the above two approaches.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Points to Remember**\n",
    "- Samples in a batch are processed **parallelly** using GPU/TPU hardware.\n",
    "- Different batches are processed **sequentially** in single-device training but can be processed parallelly in **multi-GPU setups**.\n",
    "- Choosing the right batch size is a trade-off between memory usage, training speed, and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example Workflow**\n",
    "1. **Batch Size = 64**\n",
    "2. **Forward Pass**:\n",
    "   - Compute predictions for 64 samples in parallel.\n",
    "3. **Loss Calculation**:\n",
    "   - Compute loss for the entire batch.\n",
    "4. **Backward Pass**:\n",
    "   - Calculate gradients for all model parameters.\n",
    "5. **Weight Update**:\n",
    "   - Update weights based on the computed gradients.\n",
    "\n",
    "Efficient batching is critical to training deep learning models effectively!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Are Different Batches Processed Parallelly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, different batches are generally not processed in parallel during the same training step. Here's why:\n",
    "\n",
    "- During training, the model processes one batch at a time in a sequential manner:\n",
    " 1. Perform the forward pass for a batch.\n",
    " 2. Compute the loss for that batch.\n",
    " 3. Perform the backward pass (backpropagation) to compute gradients.\n",
    " 4. Update the weights based on the gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sequence ensures that the weights are updated correctly after processing each batch, which is necessary for the model to learn progressively.\n",
    "\n",
    "**Why Not Parallel Processing for Batches?**\n",
    "\n",
    "- Dependency on Weights: The computation for the next batch depends on the updated weights from the previous batch. If batches were processed in parallel, the updates to weights could conflict or become inconsistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Parallelism Can Still Be Achieved Between Batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Multiple GPUs or TPUs (Distributed Training):**\n",
    "\n",
    "- Each GPU processes a separate batch in parallel.\n",
    "- Gradients from all GPUs are combined (via techniques like gradient aggregation) to update a shared set of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Pipeline Parallelism:**\n",
    "\n",
    "- Different stages of the model (e.g., layers) are distributed across multiple devices.\n",
    "- While one batch is being processed by earlier layers, another batch can be processed by later layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
